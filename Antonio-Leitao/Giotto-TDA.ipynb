{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# **Giotto-TDA Challenge**\n",
    "# Predicting Volcano Eruption\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary:**In this approach we show how TDA can be an effective tool for regression on **multivariate** time series analysis. We compare the TDA approach with a standard baseline approach and after show that a respectable performance can be achieved by merging both. We validate our results by comparing with the Kaggle leaderboard.\n",
    "\n",
    "**Data:** We'll be with the sensor data of several volcanoes, the objective is to predict the time untill eruption based on each volcano's sensor data. For runtime reasons we will be working with only 1/9 of the data (the full dataset is 20Gb). The full data along with the competition can be found here: https://www.kaggle.com/c/predict-volcanic-eruptions-ingv-oe/overview\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **I.**  The Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Earthquakes are devastating fenomena whose damage is not only human but material. The most challenging aspects of sismic behaviour is its unpredictability. But what is it was possible to predict earthquakes in advance as such as other environmental occurences such as the weather? Current estimates are only reliable a couple of minutes in advance and they usually fail at longer-term predictions.\n",
    "\n",
    "Italy's Istituto Nazionale di Geofisica e Vulcanologia (INGV), with its focus on geophysics and volcanology, has issued a challenge regarding this task. (https://www.kaggle.com/c/predict-volcanic-eruptions-ingv-oe/overview).\n",
    "\n",
    "\n",
    "**Task Description:** To predict \"time to eruption” by surveying volcanic tremors from seismic signals. \n",
    "\n",
    "**Data Description:** The data has 1000 volcanoes, each volcano has 10 sensors. Each sensor is a time-series data. The objective is to predict the time it will take for the volcano to erupt given the data in each sensor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/giotto_first_fig.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fig.1 Illustration of a single observation of the dataset. For each volcano there are 10 sensors, each sensor is a time-series."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **II.** Libraries "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use tensorflow and xgboost for regression (aside from the regular libraries such as pandas, numpy and giotto-tda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install xgboost tensorflow sklearn tensorflow-addons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data wrangling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "#TDA\n",
    "from gtda.diagrams import PersistenceEntropy, Scaler, PairwiseDistance, Amplitude\n",
    "from gtda.homology import VietorisRipsPersistence\n",
    "from gtda.metaestimators import CollectionTransformer\n",
    "from gtda.pipeline import Pipeline\n",
    "from gtda.time_series import TakensEmbedding, PearsonDissimilarity\n",
    "from gtda.plotting import plot_diagram\n",
    "\n",
    "\n",
    "#Benchmarking\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as bk\n",
    "import tensorflow.keras.layers as ly\n",
    "import tensorflow.keras.models as ml\n",
    "from tensorflow.keras.callbacks import EarlyStopping,ReduceLROnPlateau\n",
    "import tensorflow_addons as tfa\n",
    "import xgboost\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **III.** The Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is just 1/8 of the whole dataset. We download it directly from kaggle but for that we need its API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting kaggle\n",
      "  Using cached kaggle-1.5.9.tar.gz (58 kB)\n",
      "Requirement already satisfied: six>=1.10 in /home/antonio/.local/lib/python3.6/site-packages (from kaggle) (1.15.0)\n",
      "Requirement already satisfied: certifi in /usr/lib/python3/dist-packages (from kaggle) (2018.1.18)\n",
      "Requirement already satisfied: python-dateutil in /home/antonio/.local/lib/python3.6/site-packages (from kaggle) (2.8.1)\n",
      "Requirement already satisfied: requests in /home/antonio/.local/lib/python3.6/site-packages (from kaggle) (2.24.0)\n",
      "Collecting tqdm\n",
      "  Using cached tqdm-4.51.0-py2.py3-none-any.whl (70 kB)\n",
      "Collecting python-slugify\n",
      "  Using cached python-slugify-4.0.1.tar.gz (11 kB)\n",
      "Collecting slugify\n",
      "  Using cached slugify-0.0.1.tar.gz (1.2 kB)\n",
      "Requirement already satisfied: urllib3 in /usr/lib/python3/dist-packages (from kaggle) (1.22)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/lib/python3/dist-packages (from requests->kaggle) (2.6)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/lib/python3/dist-packages (from requests->kaggle) (3.0.4)\n",
      "Collecting text-unidecode>=1.3\n",
      "  Using cached text_unidecode-1.3-py2.py3-none-any.whl (78 kB)\n",
      "Building wheels for collected packages: kaggle, python-slugify, slugify\n",
      "  Building wheel for kaggle (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for kaggle: filename=kaggle-1.5.9-py3-none-any.whl size=73265 sha256=42aa4ee64515118260e2a7581ecfeaa90dd3422ff1fd65864a86eef20e43fdac\n",
      "  Stored in directory: /home/antonio/.cache/pip/wheels/9d/50/3d/2644504bb1e8c782f3fef5984f03d76fc4a74698fdec128b29\n",
      "  Building wheel for python-slugify (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for python-slugify: filename=python_slugify-4.0.1-py2.py3-none-any.whl size=6767 sha256=e3e138a8db0cb53d99bde1db4dbe917f3481614a95a9c30ee8a0eb4d8147e6eb\n",
      "  Stored in directory: /home/antonio/.cache/pip/wheels/72/e6/db/122611605e60148f54ee2abaca98b2bbeafc6e22486a867bad\n",
      "  Building wheel for slugify (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for slugify: filename=slugify-0.0.1-py3-none-any.whl size=1908 sha256=9efbb25090dbe07d5f46c03c90e95ba349bad4f30b602ffe445dfa8b0536ed10\n",
      "  Stored in directory: /home/antonio/.cache/pip/wheels/7d/51/b0/c584cbdd0a8fc685d68677e58cde93814cbbc7fd9867fb5fe6\n",
      "Successfully built kaggle python-slugify slugify\n",
      "Installing collected packages: tqdm, text-unidecode, python-slugify, slugify, kaggle\n",
      "Successfully installed kaggle-1.5.9 python-slugify-4.0.1 slugify-0.0.1 text-unidecode-1.3 tqdm-4.51.0\n",
      "\u001b[33mWARNING: You are using pip version 20.1.1; however, version 20.2.4 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "mkdir: cannot create directory ‘/home/antonio/.kaggle’: File exists\n"
     ]
    }
   ],
   "source": [
    "!{sys.executable} -m pip install kaggle\n",
    "!mkdir ~/.kaggle \n",
    "!cp kaggle.json ~/.kaggle/\n",
    "!chmod 600 ~/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading volcano-data.zip to /home/antonio/test_run/unbound/Giotto\n",
      "100%|█████████████████████████████████████▉| 1.15G/1.15G [02:31<00:00, 8.41MB/s]\n",
      "100%|██████████████████████████████████████| 1.15G/1.15G [02:31<00:00, 8.16MB/s]\n"
     ]
    }
   ],
   "source": [
    "#Download the data\n",
    "!kaggle datasets download -d antnioleito/volcano-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unzip the data\n",
    "import zipfile\n",
    "with zipfile.ZipFile('volcano-data.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall('data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "volcano_ids = [item for item in os.listdir('data/')]\n",
    "labels = pd.read_csv('train.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#2F4F4F\">**volcano_ids**</span> has the id of each volcano, each volcano has a dataframe of 60000 entries for each of its 10 sensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sensor_1</th>\n",
       "      <th>sensor_2</th>\n",
       "      <th>sensor_3</th>\n",
       "      <th>sensor_4</th>\n",
       "      <th>sensor_5</th>\n",
       "      <th>sensor_6</th>\n",
       "      <th>sensor_7</th>\n",
       "      <th>sensor_8</th>\n",
       "      <th>sensor_9</th>\n",
       "      <th>sensor_10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-534.0</td>\n",
       "      <td>-1217.0</td>\n",
       "      <td>-151.0</td>\n",
       "      <td>244.0</td>\n",
       "      <td>412.0</td>\n",
       "      <td>438.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>-31.0</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-364.0</td>\n",
       "      <td>-1160.0</td>\n",
       "      <td>-222.0</td>\n",
       "      <td>339.0</td>\n",
       "      <td>318.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>143.0</td>\n",
       "      <td>323.0</td>\n",
       "      <td>-137.0</td>\n",
       "      <td>111.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-71.0</td>\n",
       "      <td>-1068.0</td>\n",
       "      <td>-228.0</td>\n",
       "      <td>514.0</td>\n",
       "      <td>315.0</td>\n",
       "      <td>-152.0</td>\n",
       "      <td>184.0</td>\n",
       "      <td>-96.0</td>\n",
       "      <td>-246.0</td>\n",
       "      <td>-54.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-113.0</td>\n",
       "      <td>-1059.0</td>\n",
       "      <td>-259.0</td>\n",
       "      <td>627.0</td>\n",
       "      <td>324.0</td>\n",
       "      <td>-188.0</td>\n",
       "      <td>196.0</td>\n",
       "      <td>-22.0</td>\n",
       "      <td>-335.0</td>\n",
       "      <td>-377.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-106.0</td>\n",
       "      <td>-1038.0</td>\n",
       "      <td>-311.0</td>\n",
       "      <td>760.0</td>\n",
       "      <td>267.0</td>\n",
       "      <td>-275.0</td>\n",
       "      <td>234.0</td>\n",
       "      <td>-188.0</td>\n",
       "      <td>-417.0</td>\n",
       "      <td>-629.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sensor_1  sensor_2  sensor_3  sensor_4  sensor_5  sensor_6  sensor_7  \\\n",
       "0    -534.0   -1217.0    -151.0     244.0     412.0     438.0      67.0   \n",
       "1    -364.0   -1160.0    -222.0     339.0     318.0      64.0     143.0   \n",
       "2     -71.0   -1068.0    -228.0     514.0     315.0    -152.0     184.0   \n",
       "3    -113.0   -1059.0    -259.0     627.0     324.0    -188.0     196.0   \n",
       "4    -106.0   -1038.0    -311.0     760.0     267.0    -275.0     234.0   \n",
       "\n",
       "   sensor_8  sensor_9  sensor_10  \n",
       "0      60.0     -31.0       15.0  \n",
       "1     323.0    -137.0      111.0  \n",
       "2     -96.0    -246.0      -54.0  \n",
       "3     -22.0    -335.0     -377.0  \n",
       "4    -188.0    -417.0     -629.0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = pd.read_csv('data/'+volcano_ids[0])\n",
    "temp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#2F4F4F\">**labels**</span> has the id of each volcano and the \"time to erupt\" our regression target. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>segment_id</th>\n",
       "      <th>time_to_eruption</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1136037770</td>\n",
       "      <td>12262005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1969647810</td>\n",
       "      <td>32739612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1895879680</td>\n",
       "      <td>14965999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2068207140</td>\n",
       "      <td>26469720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>192955606</td>\n",
       "      <td>31072429</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   segment_id  time_to_eruption\n",
       "0  1136037770          12262005\n",
       "1  1969647810          32739612\n",
       "2  1895879680          14965999\n",
       "3  2068207140          26469720\n",
       "4   192955606          31072429"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly we define a preprocessing function that fills the nan values with a window average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(volcano_name):\n",
    "    series = pd.read_csv('data/'+volcano_name)\n",
    "    return series.fillna(series.rolling(10,min_periods=1).mean()).dropna(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **IV.** A Topological Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our approach is to do a Taken's Embedding for each one of the sensors. After, we compute the Vietoris-Tips filtration and extract persistence summaries: Entropy and Amplitude"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **IV.a Takens' embedding on multivariate time series**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/giotto_sec_fig.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a The pipeline\n",
    "embedding_dimension = 10\n",
    "embedding_time_delay = 1\n",
    "stride = 100\n",
    "\n",
    "#Takens Embedding\n",
    "embedder = TakensEmbedding(time_delay=embedding_time_delay,\n",
    "                           dimension=embedding_dimension,\n",
    "                           stride=stride)\n",
    "#Persistent Homology\n",
    "persistence = VietorisRipsPersistence(homology_dimensions=[0, 1], n_jobs=-1)\n",
    "steps = [(\"embedder\", embedder),\n",
    "         (\"persistence\", persistence)]\n",
    "transfomer = Pipeline(steps)\n",
    "\n",
    "#After calculate also the amplitude and Entropy\n",
    "amp = Amplitude()\n",
    "ent = PersistenceEntropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/antonio/.local/lib/python3.6/site-packages/gtda/homology/simplicial.py:209: DataDimensionalityWarning:\n",
      "\n",
      "Input array X has X.shape[1] == X.shape[2]. This is consistent with a collection of distance/adjacency matrices, but the input is being treated as a collection of vectors in Euclidean space.\n",
      "\n",
      "/home/antonio/.local/lib/python3.6/site-packages/gtda/homology/simplicial.py:271: DataDimensionalityWarning:\n",
      "\n",
      "Input array X has X.shape[1] == X.shape[2]. This is consistent with a collection of distance/adjacency matrices, but the input is being treated as a collection of vectors in Euclidean space.\n",
      "\n",
      "/home/antonio/.local/lib/python3.6/site-packages/gtda/homology/simplicial.py:209: DataDimensionalityWarning:\n",
      "\n",
      "Input array X has X.shape[1] == X.shape[2]. This is consistent with a collection of distance/adjacency matrices, but the input is being treated as a collection of vectors in Euclidean space.\n",
      "\n",
      "/home/antonio/.local/lib/python3.6/site-packages/gtda/homology/simplicial.py:271: DataDimensionalityWarning:\n",
      "\n",
      "Input array X has X.shape[1] == X.shape[2]. This is consistent with a collection of distance/adjacency matrices, but the input is being treated as a collection of vectors in Euclidean space.\n",
      "\n",
      "/home/antonio/.local/lib/python3.6/site-packages/gtda/homology/simplicial.py:209: DataDimensionalityWarning:\n",
      "\n",
      "Input array X has X.shape[1] == X.shape[2]. This is consistent with a collection of distance/adjacency matrices, but the input is being treated as a collection of vectors in Euclidean space.\n",
      "\n",
      "/home/antonio/.local/lib/python3.6/site-packages/gtda/homology/simplicial.py:271: DataDimensionalityWarning:\n",
      "\n",
      "Input array X has X.shape[1] == X.shape[2]. This is consistent with a collection of distance/adjacency matrices, but the input is being treated as a collection of vectors in Euclidean space.\n",
      "\n",
      "/home/antonio/.local/lib/python3.6/site-packages/gtda/homology/simplicial.py:209: DataDimensionalityWarning:\n",
      "\n",
      "Input array X has X.shape[1] == X.shape[2]. This is consistent with a collection of distance/adjacency matrices, but the input is being treated as a collection of vectors in Euclidean space.\n",
      "\n",
      "/home/antonio/.local/lib/python3.6/site-packages/gtda/homology/simplicial.py:271: DataDimensionalityWarning:\n",
      "\n",
      "Input array X has X.shape[1] == X.shape[2]. This is consistent with a collection of distance/adjacency matrices, but the input is being treated as a collection of vectors in Euclidean space.\n",
      "\n",
      "/home/antonio/.local/lib/python3.6/site-packages/gtda/homology/simplicial.py:209: DataDimensionalityWarning:\n",
      "\n",
      "Input array X has X.shape[1] == X.shape[2]. This is consistent with a collection of distance/adjacency matrices, but the input is being treated as a collection of vectors in Euclidean space.\n",
      "\n",
      "/home/antonio/.local/lib/python3.6/site-packages/gtda/homology/simplicial.py:271: DataDimensionalityWarning:\n",
      "\n",
      "Input array X has X.shape[1] == X.shape[2]. This is consistent with a collection of distance/adjacency matrices, but the input is being treated as a collection of vectors in Euclidean space.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Apply the takens embedding and get the persistence diagrams for every volcano (diags)\n",
    "#This is the most time-consuming part which should take around 10 minutes.\n",
    "\n",
    "diags=[]\n",
    "for volcano in volcano_ids[:-1]:\n",
    "    volcano_data = preprocess(volcano)\n",
    "    diags.append(transfomer.fit_transform(volcano_data.values.T) )\n",
    "\n",
    "#Get the H0 and H1 entropy and amplitude of each peristince diagram of each volcano   \n",
    "amplitude=[]\n",
    "entropy=[]\n",
    "ws = []\n",
    "for diag in diags:\n",
    "    amplitude.append(amp.fit_transform(diag))\n",
    "    entropy.append(ent.fit_transform(diag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gtda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3.0\n"
     ]
    }
   ],
   "source": [
    "print(gtda.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **IV.b Build new features.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/giotto_trd_fig.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now build a new dataset. For each observation (volcano) we have the topological variables just calculated. Lastly we bring the \"time_to_erupt\" variable, our regression target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge H0 and H1 entropy and amplitude into columns\n",
    "X_topo = np.c_[np.array(amplitude)[:,:,0],np.array(amplitude)[:,:,1],\n",
    "               np.array(entropy)[:,:,0],np.array(entropy)[:,:,1]]\n",
    "\n",
    "#Grab the regression target\n",
    "y=np.array([labels[labels['segment_id']==np.int(name[:-4])]['time_to_eruption'].values[0] for name in volcano_ids[:-1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **V.** Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createANN(X):\n",
    "    model = ml.Sequential()\n",
    "    model.add(ly.Input(X.shape[1]))\n",
    "    model.add(ly.BatchNormalization())\n",
    "    model.add(tfa.layers.WeightNormalization(ly.Dense(1000,activation='relu')))\n",
    "    model.add(ly.BatchNormalization())\n",
    "    model.add(ly.Dropout(0.7))\n",
    "    model.add(tfa.layers.WeightNormalization(ly.Dense(1,activation='relu')))\n",
    "\n",
    "\n",
    "    model.compile(optimizer=tfa.optimizers.AdamW(lr = 1, weight_decay = 1e-5, clipvalue = 900),loss='mean_absolute_error')\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark(X,y, k=10):\n",
    "    kf = KFold(n_splits=k)\n",
    "    kf.get_n_splits(X)\n",
    "    \n",
    "    ann_scores =[]\n",
    "    xgb_scores = []\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train, X_val = X[train_index], X[test_index]\n",
    "        y_train, y_val = y[train_index], y[test_index]\n",
    "        \n",
    "        #ANN\n",
    "        cb_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, min_lr=1e-7, patience=2, verbose=0, mode='min')\n",
    "        cb_early = EarlyStopping(monitor=\"val_loss\", mode=\"min\", restore_best_weights=True, patience= 5, verbose = 0)\n",
    "        model=createANN(X)\n",
    "        model.fit(X_train,y_train,batch_size=8,epochs=600,verbose=0,validation_data=(X_val,y_val),callbacks=[cb_lr,cb_early])\n",
    "\n",
    "        #XGBOOST\n",
    "        model1 = xgboost.XGBRegressor(n_estimators=100000,max_depth=8,learning_rate=0.05,alpha=0.1,SUBSAMPLE=0.6)\n",
    "        eval_set = [(X_val, y_val)]\n",
    "        model1.fit(X_train, y_train,early_stopping_rounds=5,eval_metric='mae', eval_set=eval_set, verbose=False)\n",
    "        \n",
    "        ann_scores.append(np.mean(np.abs(model.predict(X_val)-y_val)))\n",
    "        xgb_scores.append(np.mean(np.abs(model1.predict(X_val)-y_val)))\n",
    "    \n",
    "    return np.mean(ann_scores), np.mean(xgb_scores)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[02:11:57] WARNING: ../src/learner.cc:516: \n",
      "Parameters: { SUBSAMPLE } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[02:12:01] WARNING: ../src/learner.cc:516: \n",
      "Parameters: { SUBSAMPLE } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[02:12:04] WARNING: ../src/learner.cc:516: \n",
      "Parameters: { SUBSAMPLE } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[02:12:07] WARNING: ../src/learner.cc:516: \n",
      "Parameters: { SUBSAMPLE } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[02:12:10] WARNING: ../src/learner.cc:516: \n",
      "Parameters: { SUBSAMPLE } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[02:12:13] WARNING: ../src/learner.cc:516: \n",
      "Parameters: { SUBSAMPLE } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[02:12:16] WARNING: ../src/learner.cc:516: \n",
      "Parameters: { SUBSAMPLE } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[02:12:19] WARNING: ../src/learner.cc:516: \n",
      "Parameters: { SUBSAMPLE } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[02:12:22] WARNING: ../src/learner.cc:516: \n",
      "Parameters: { SUBSAMPLE } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[02:12:25] WARNING: ../src/learner.cc:516: \n",
      "Parameters: { SUBSAMPLE } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ann, xgb = benchmark(X_topo, y, k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network 10-fold MAE: 11958279.017435\n",
      "XGBoost 10-fold MAE: 6166860.9578125\n"
     ]
    }
   ],
   "source": [
    "print(f'Neural Network 10-fold MAE: {ann}')\n",
    "print(f'XGBoost 10-fold MAE: {xgb}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **VI.** Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we compare out method with the baseline one. This approach was taked from the most voted baseline notebook at this kaggle competition: https://www.kaggle.com/soheild91/ingv-nn-xgb-baseline\n",
    "\n",
    "The idea is similar but the features extracted from each sensor are not the topological ones but simple statistical ones. While we took one 2 features per sensor, the standard baseline takes 12 new features per sensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_features=12\n",
    "base_data=np.empty((len(volcano_ids[:-1]),new_features*10))\n",
    "for i_ in range(len(volcano_ids[:-1])):\n",
    "    the_df=preprocess(volcano_ids[i_])\n",
    "    base_data[i_,:]=np.concatenate((the_df.abs().mean().to_numpy(),\n",
    "                                    the_df.std().to_numpy(),\n",
    "                                    the_df.mean().to_numpy(),\n",
    "                                    the_df.var().to_numpy(),\n",
    "                                    the_df.min().to_numpy(),\n",
    "                                    the_df.max().to_numpy(),\n",
    "                                    the_df.median().to_numpy(),\n",
    "                                    the_df.quantile([0.1,0.25,0.5,0.75,0.9]).to_numpy().reshape(1,-1)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[02:14:27] WARNING: ../src/learner.cc:516: \n",
      "Parameters: { SUBSAMPLE } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[02:14:32] WARNING: ../src/learner.cc:516: \n",
      "Parameters: { SUBSAMPLE } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[02:14:36] WARNING: ../src/learner.cc:516: \n",
      "Parameters: { SUBSAMPLE } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[02:14:41] WARNING: ../src/learner.cc:516: \n",
      "Parameters: { SUBSAMPLE } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[02:14:47] WARNING: ../src/learner.cc:516: \n",
      "Parameters: { SUBSAMPLE } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[02:14:57] WARNING: ../src/learner.cc:516: \n",
      "Parameters: { SUBSAMPLE } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[02:15:01] WARNING: ../src/learner.cc:516: \n",
      "Parameters: { SUBSAMPLE } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[02:15:06] WARNING: ../src/learner.cc:516: \n",
      "Parameters: { SUBSAMPLE } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[02:15:11] WARNING: ../src/learner.cc:516: \n",
      "Parameters: { SUBSAMPLE } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[02:15:16] WARNING: ../src/learner.cc:516: \n",
      "Parameters: { SUBSAMPLE } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[02:15:20] WARNING: ../src/learner.cc:516: \n",
      "Parameters: { SUBSAMPLE } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[02:15:25] WARNING: ../src/learner.cc:516: \n",
      "Parameters: { SUBSAMPLE } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[02:15:31] WARNING: ../src/learner.cc:516: \n",
      "Parameters: { SUBSAMPLE } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[02:15:37] WARNING: ../src/learner.cc:516: \n",
      "Parameters: { SUBSAMPLE } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[02:15:43] WARNING: ../src/learner.cc:516: \n",
      "Parameters: { SUBSAMPLE } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[02:15:49] WARNING: ../src/learner.cc:516: \n",
      "Parameters: { SUBSAMPLE } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[02:15:54] WARNING: ../src/learner.cc:516: \n",
      "Parameters: { SUBSAMPLE } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[02:15:59] WARNING: ../src/learner.cc:516: \n",
      "Parameters: { SUBSAMPLE } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[02:16:05] WARNING: ../src/learner.cc:516: \n",
      "Parameters: { SUBSAMPLE } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[02:16:10] WARNING: ../src/learner.cc:516: \n",
      "Parameters: { SUBSAMPLE } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stats = benchmark(base_data,y,k=10)\n",
    "together = benchmark(np.c_[X_topo,base_data],y,k=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **VII.** Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Just Topological features\n",
      "Neural Network 10-fold MAE: 11958279.017435\n",
      "XGBoost 10-fold MAE: 6166860.9578125\n",
      "\n",
      "\n",
      "Just Simple Statistics\n",
      "Neural Network 10-fold MAE: 12119103.21565\n",
      "XGBoost 10-fold MAE: 5642745.1707500005\n",
      "\n",
      "\n",
      "Both together\n",
      "Neural Network 10-fold MAE: 12108888.428183125\n",
      "XGBoost 10-fold MAE: 5364914.2945\n"
     ]
    }
   ],
   "source": [
    "print('Just Topological features')\n",
    "print(f'Neural Network 10-fold MAE: {ann}')\n",
    "print(f'XGBoost 10-fold MAE: {xgb}')\n",
    "print('\\n')\n",
    "print('Just Simple Statistics')\n",
    "print(f'Neural Network 10-fold MAE: {stats[0]}')\n",
    "print(f'XGBoost 10-fold MAE: {stats[1]}')\n",
    "print('\\n')\n",
    "print('Both together')\n",
    "print(f'Neural Network 10-fold MAE: {together[0]}')\n",
    "print(f'XGBoost 10-fold MAE: {together[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note that we are only using 1/8 of the whole dataset we still manage to get a very competitive approach. Note that the simple statistical approach has 6 times more features. The results are nothing short of impressive specially when considering both approaches combined."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the kaggle leaderboard for comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/giotto_leaderboard.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authors: António Leitão and Giovanni Petri"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
